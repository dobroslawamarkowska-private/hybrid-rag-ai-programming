# Advanced RAG â€“ Dokumentacja architektury

Pipeline RAG (Retrieval Augmented Generation) dla dokumentacji Docker. Zbudowany w LangGraph z etapami: **ingest** (multi-turn), pre-retrieval, retrieval, grader + refinement, post-retrieval, **summarize_conversation**, generate.

â†’ Instalacja i uruchomienie: [README](../README.md)

---

## PrzepÅ‚yw â€“ diagram gÅ‚Ã³wny (branch token-savings-conversation-hygiene)

```
  messages (HumanMessage)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ingest      â”‚  query = messages[-1].content
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pre-Retrieval  â”‚â”€â”€â”€â”€â–ºâ”‚  Retrieval  â”‚â”€â”€â”€â”€â–ºâ”‚  Check & Refine     â”‚
â”‚  query expansionâ”‚     â”‚  embeddings â”‚     â”‚  (grader 0â€“1)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                              â”‚                              â”‚
                        â–¼                              â”‚                              â–¼
                 score â‰¥ 0.50 (OK)                     â”‚                        score < 0.50 (BAD)
                        â”‚                              â”‚                              â”‚ retry (max 1Ã—)
                        â–¼                              â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Post-Retrieval  â”‚â”€â”€â”€â”€â–ºâ”‚ Summarize conversationâ”‚â”€â”€â”€â”€â–ºâ”‚  Generate   â”‚
                 â”‚ rerank, context â”‚     â”‚ (gdy messages > 1500  â”‚     â”‚ (context +  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  tokenÃ³w)             â”‚     â”‚  summary)   â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                                             â”‚
                                                                             â–¼
                                                                          Output
```

---

## Diagram Mermaid â€“ flowchart

```mermaid
flowchart TB
    M[messages]
    INGEST[Ingest<br/>query z messages[-1]]
    PR[Pre-Retrieval<br/>query expansion]
    R[Retrieval<br/>embeddings + vector search]
    CR[Check & Refine<br/>grader 0â€“1]
    POST[Post-Retrieval<br/>rerank, build context]
    SUM[Summarize conversation<br/>gdy messages > 1500 tokenÃ³w]
    GEN[Generate<br/>context + summary + messages]
    OUT[Output]

    M --> INGEST
    INGEST --> PR
    PR --> R
    R --> CR
    CR -->|score â‰¥ 0.5| POST
    CR -->|score < 0.5, retry| R
    POST --> SUM
    SUM --> GEN
    GEN --> OUT
```

---

## Diagram workflow â€“ kolorowy UML (Mermaid)

Diagram aktywnoÅ›ci w stylu UML: kolorowe wÄ™zÅ‚y, rÃ³Å¼ne ksztaÅ‚ty, wyraÅºne decyzje.

```mermaid
flowchart TB
    subgraph input [" ğŸ“¥ WejÅ›cie "]
        M(("messages<br/>HumanMessage"))
    end

    subgraph processing [" âš™ï¸ Pipeline RAG "]
        INGEST[Ingest<br/>query = messages[-1]]
        PR[Pre-Retrieval<br/>1â€“3 expanded queries]
        R[Retrieval<br/>embedding + vector search]
        POST[Post-Retrieval<br/>rerank, 6 chunkÃ³w]
        SUM[Summarize conversation<br/>messages > 1500 tok.]
        GEN[Generate<br/>context + summary + msgs]
    end

    subgraph decision [" â“ Decyzja "]
        CR{{"Grader<br/>score 0â€“1"}}
    end

    subgraph output [" ğŸ“¤ WyjÅ›cie "]
        OUT(("answer"))
    end

    M --> INGEST
    INGEST --> PR
    PR --> R
    R --> CR
    CR -->|"â‰¥ 0.5 OK"| POST
    CR -->|"< 0.5 retry (max 1Ã—)"| R
    POST --> SUM
    SUM --> GEN
    GEN --> OUT

    classDef inputStyle fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef processStyle fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    classDef decisionStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef outputStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px

    class M,OUT inputStyle,outputStyle
    class INGEST,PR,R,POST,SUM,GEN processStyle
    class CR decisionStyle
```

---

## Diagram Mermaid â€“ sekwencja (z retry)

```mermaid
sequenceDiagram
    participant U as User
    participant INGEST as Ingest
    participant PR as Pre-Retrieval
    participant R as Retrieval
    participant CR as Check & Refine
    participant POST as Post-Retrieval
    participant SUM as Summarize
    participant GEN as Generate

    U->>INGEST: messages
    INGEST->>INGEST: query = messages[-1].content
    INGEST->>PR: query
    PR->>PR: gpt-4o: 1â€“3 expanded queries
    PR->>R: expanded_queries

    R->>R: embeddings (OpenRouter)
    R->>R: similarity search, dedupe
    R->>CR: raw_docs

    alt score â‰¥ 0.5 (docs OK)
        CR->>POST: raw_docs
    else score < 0.5 (docs BAD)
        CR->>CR: gpt-4o: SCORE + REFINED query
        CR->>R: refined_query (retry)
        R->>CR: raw_docs
        CR->>POST: raw_docs
    end

    POST->>POST: rerank, build context
    POST->>SUM: context, messages
    alt messages > 1500 tokenÃ³w
        SUM->>SUM: LLM: summarize starsze wiadomoÅ›ci, RemoveMessage
    end
    SUM->>GEN: context + summary + messages
    GEN->>GEN: trim_messages(max 4096), gpt-4o: answer
    GEN->>U: answer
```

---

## Opis etapÃ³w

| Etap | Model | Opis |
|------|-------|------|
| **Ingest** | â€” | Pobiera `query` z ostatniej wiadomoÅ›ci uÅ¼ytkownika (`messages[-1].content`). WejÅ›cie: `messages` (add_messages). |
| **Pre-Retrieval** | SMART_LLM | Zamiana pytania na 1â€“3 zapytania wyszukiwania (routing, rewriting, expansion). |
| **Retrieval** | EMBEDDING_MODEL | **Orchestratorâ€“workers**: rÃ³wnolegÅ‚e workery (ThreadPoolExecutor) â€“ kaÅ¼dy worker wykonuje embedding + wyszukiwanie dla jednego expanded query. Konfiguracja: `RETRIEVAL_MAX_WORKERS` w `config.py`. |
| **Check & Refine** | GRADER_LLM | **Grader 0.00â€“1.00**: ocena relewancji chunkÃ³w. Score â‰¥ 0.50 â†’ OK. Score < 0.50 â†’ LLM poprawia pytanie i retry retrieval (max 1Ã—). |
| **Post-Retrieval** | â€” | Rerank, deduplikacja, budowanie kontekstu (do 6 chunkÃ³w). |
| **Summarize conversation** | SMART_LLM | Gdy `messages` > 1500 tokenÃ³w, LLM podsumowuje starsze wiadomoÅ›ci; RemoveMessage usuwa je, zostajÄ… 2 ostatnie. Pattern: [Summarize messages](https://docs.langchain.com/oss/python/langgraph/add-memory#summarize-messages). |
| **Generate** | SMART_LLM | OdpowiedÅº z kontekstu RAG + `summary` (jeÅ›li jest) + `messages`. `trim_messages` ogranicza do 4096 tokenÃ³w. Przy braku dopasowania: komunikat + propozycja najbliÅ¼szej informacji. |

---

## Multi-turn i pamiÄ™Ä‡ rozmowy

- **messages** â€“ historia rozmowy (HumanMessage, AIMessage) dziÄ™ki `add_messages` (LangGraph).
- **thread_id** â€“ kaÅ¼da sesja ma osobny wÄ…tek; `ask(query, thread_id="user-123")` zachowuje historiÄ™ miÄ™dzy wywoÅ‚aniami.
- **Summarize conversation** â€“ gdy `messages` przekracza `MAX_MESSAGES_BEFORE_SUMMARY` (1500 tokenÃ³w), LLM podsumowuje starsze wiadomoÅ›ci; generate dostaje `summary` + ostatnie 2 messages.
- **Decyzja o summarization** â€“ na podstawie dÅ‚ugoÅ›ci `messages` (historia akumuluje siÄ™), nie `context` (RAG jest przebudowywany przy kaÅ¼dym pytaniu i ma staÅ‚y limit).

StaÅ‚e w `workflow.py`: `MAX_MESSAGES_BEFORE_SUMMARY`, `MAX_CONTENT_TOKENS`.

LLM i embeddingi: `config.py` (SMART_LLM_MODEL, GRADER_LLM_MODEL, EMBEDDING_MODEL). **Uwaga:** Zmiana modelu embedding wymaga przebudowy indeksu: `REBUILD_INDEX=1 python build_index.py`.

---

## Grader (Check & Refine)

Decyzja o jakoÅ›ci dokumentÃ³w:

1. **WejÅ›cie**: `query`, pierwsze 3 chunki (tytuÅ‚ + 80 znakÃ³w treÅ›ci).
2. **LLM zwraca**:
   - `SCORE: 0.00â€“1.00` (0.00 = zupeÅ‚nie nieistotne, 1.00 = idealnie istotne, 2 miejsca po przecinku),
   - `REFINED:` â€“ poprawione pytanie (gdy score < 0.50) lub oryginaÅ‚ (gdy â‰¥ 0.50).
3. **PrÃ³g** `RELEVANCE_THRESHOLD = 0.5`:
   - score â‰¥ 0.50 â†’ dalej do post_retrieval,
   - score < 0.50 â†’ retry retrieval z refined query (max 1 retry).

---

## Zachowanie przy braku dopasowania

Gdy pytanie **nie wystÄ™puje** w dokumentacji:

1. Generate **jasno informuje**, Å¼e dokÅ‚adnej informacji nie ma.
2. **Proponuje najbliÅ¼szÄ…** pasujÄ…cÄ… treÅ›Ä‡ z kontekstu.

---

## Pliki projektu

| Plik | OdpowiedzialnoÅ›Ä‡ |
|------|------------------|
| `config.py` | StaÅ‚e: CHROMA_DIR, COLLECTION_NAME, OPENROUTER_*, modele, RETRIEVAL_MAX_WORKERS (liczba rÃ³wnolegÅ‚ych retrieval workers). |
| `build_index.py` | Budowanie indeksu Chroma (uruchamiane rÄ™cznie). |
| `retriever.py` | Retriever i tool `create_docker_docs_tool()`. |
| `workflow.py` | LangGraph workflow: ingest â†’ pre_retrieval â†’ retrieval â†’ check_and_refine â†’ post_retrieval â†’ summarize_conversation â†’ generate. StaÅ‚e: MAX_MESSAGES_BEFORE_SUMMARY, MAX_CONTENT_TOKENS. |
| `eval_dataset.py` | Tworzenie datasetu LangSmith (branch langsmith-eval). |
| `eval_rag.py` | Ewaluacja RAG przez LangSmith Client (branch langsmith-eval). |
| `tests/` | Testy: jednostkowe (workflow ingest, summarize_conversation, generate, post_retrieval, retrieval, grader; build_index, eval), integracyjne (retriever, ask). `SKIP_INTEGRATION=1` pomija testy wymagajÄ…ce API. |

---

## Uruchomienie

Po instalacji zaleÅ¼noÅ›ci i utworzeniu pliku `.env` z `OPENAI_API_KEY` (zobacz [README](../README.md)):

```bash
# Aktywacja Å›rodowiska
source venv/bin/activate

# Budowanie indeksu (jednorazowo; dane z Kaggle lub ./data/)
python build_index.py

# Zapytanie przez API Pythona
python -c "from workflow import ask; print(ask('Jak zainstalowaÄ‡ Docker?'))"

# Uruchomienie z przykÅ‚adowym pytaniem
python workflow.py

# Testy â€“ wszystkie (wymaga indeksu Chroma i API OpenRouter)
python -m unittest discover tests -v

# Tylko testy jednostkowe (bez API/Chroma)
SKIP_INTEGRATION=1 python -m unittest discover tests -v
```

---

## OpenRouter

LLM i embeddingi korzystajÄ… z **OpenRouter** â€“ moÅ¼esz wybieraÄ‡ rÃ³Å¼ne modele (OpenAI, Anthropic, Google itd.). W `.env`:

```
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
```

W `config.py` ustaw `EMBEDDING_MODEL`, `SMART_LLM_MODEL` i `GRADER_LLM_MODEL` w formacie `provider/model` (np. `anthropic/claude-3.5-sonnet`, `google/gemini-pro`).

---

## Observability (LangSmith)

Projekt korzysta z **LangSmith** do Å›ledzenia wywoÅ‚aÅ„ LLM i workflow. Skonfiguruj `.env`:

```
LANGSMITH_TRACING=true
LANGSMITH_API_KEY=lsv2_pt_...
LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com   # opcjonalnie dla EU
LANGSMITH_PROJECT=hybrid-rag
```

Przy wÅ‚Ä…czonym tracingu tracy sÄ… wysyÅ‚ane do [smith.langchain.com](https://smith.langchain.com).

---

## Ewaluacja (branch langsmith-eval)

Skrypty `eval_dataset.py` i `eval_rag.py` umoÅ¼liwiajÄ… ewaluacjÄ™ RAG na datasetcie LangSmith:

- **eval_dataset.py** â€“ tworzy dataset "Docker RAG Eval" z 8 pytaniami, expected_keywords i expected_answer.
- **eval_rag.py** â€“ uruchamia workflow na datasetcie, evaluatory:
  - `answer_not_empty` â€“ heurystyka (0 tokenÃ³w)
  - `expected_keywords_present` â€“ heurystyka (0 tokenÃ³w)
  - `qa_correctness` â€“ LLM-as-judge, opcja `--llm-judge` (dodatkowe wywoÅ‚ania LLM)

Wyniki eksportowane do LangSmith; moÅ¼na porÃ³wnaÄ‡ eksperymenty i pobraÄ‡ CSV.

---

## Trace mode (opcja uruchomieniowa)

Z flagÄ… `--trace` workflow generuje dwa dokumenty markdown:

1. **answer.md** â€“ Å‚adna odpowiedÅº w formacie MD.
2. **flow_trace.md** â€“ opis krok po kroku przepÅ‚ywu:
   - kaÅ¼dy wÄ™zeÅ‚ (ingest, pre_retrieval, retrieval, check_and_refine, post_retrieval, summarize_conversation, generate),
   - uÅ¼yty model (np. openai/gpt-4o, openai/text-embedding-3-small),
   - liczba wywoÅ‚aÅ„ API,
   - szczegÃ³Å‚y (np. "Skipped (no docs)" lub "Skipped (retry limit reached, passing to post_retrieval)" dla check_and_refine),
   - podsumowanie wywoÅ‚aÅ„ per model.

```bash
python workflow.py --trace -q "Jak zainstalowaÄ‡ Docker?" -o ./output
```

Programowo: `ask(query, trace=True)` zwraca `(answer_md, flow_trace_md)`.

---

## Debug

Workflow wypisuje `[DEBUG ...]` dla route_query, pre_retrieval, retrieval, check_and_refine, post_retrieval (wejÅ›cie/wyjÅ›cie, route, score, expanded_queries).
